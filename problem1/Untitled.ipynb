{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_docs_and_labels(file_name):\n",
    "    \"\"\"\n",
    "    :param file_name:\n",
    "    :return: Return tokenized 2d string matrix\n",
    "    \"\"\"\n",
    "    # Read training documents\n",
    "    # with codecs.open(file_name, 'r', 'utf-8-sig') as raw_text:\n",
    "    with open(file_name, 'r') as raw_text:\n",
    "        # raw_text = raw_text.encode('ascii', 'ignore')\n",
    "        lines = raw_text.readlines()\n",
    "\n",
    "    # Tokenization with NLTK\n",
    "    docs = [nltk.tokenize.word_tokenize(line) for line in lines]\n",
    "    # docs = [nltk.tokenize.word_tokenize(line.decode('utf-8')) for line in lines]\n",
    "#     docs = [l.split() for l in lines]\n",
    "\n",
    "    # remove all tokens that are not alphabetic\n",
    "    docs = [list(filter(lambda word: word.isalpha(), doc)) for doc in docs if True]\n",
    "\n",
    "    min_len = 4\n",
    "    docs = filterLen(docs, min_len)\n",
    "    \n",
    "    return [doc[1:] for doc in docs], [doc[:1] for doc in docs]\n",
    "\n",
    "\n",
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "\n",
    "\n",
    "def get_tf_idf_matrix(docs):\n",
    "    \"\"\"\n",
    "    :param docs:\n",
    "    :return: The term frequency - inversed document frequency representation of each document\n",
    "    \"\"\"\n",
    "    tf_matrix = get_tf_matrix(docs)\n",
    "\n",
    "    scale_with_idf(tf_matrix)\n",
    "\n",
    "    l2_normalize(tf_matrix)\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def get_tf_matrix(docs):\n",
    "    \"\"\"\n",
    "    :param docs:\n",
    "    :return: The term frequencies of the documents in csr format\n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows + 1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k, _ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j, k in enumerate(keys):\n",
    "            ind[j + n] = idx[k]\n",
    "            val[j + n] = cnt[k]\n",
    "        ptr[i + 1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "\n",
    "    tf_matrix = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    tf_matrix.sort_indices()\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def scale_with_idf(tf_matrix):\n",
    "    \"\"\"\n",
    "    :param tf_matrix: term frequencies of th corpus\n",
    "    :return: Scale term frequency with inversed document frequency\n",
    "    \"\"\"\n",
    "    # document word frequency\n",
    "    df = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for w in set(d):\n",
    "            df[w] += 1\n",
    "\n",
    "    nrows = tf_matrix.shape[0]\n",
    "    nnz = tf_matrix.nnz\n",
    "    ind, val, ptr = tf_matrix.indices, tf_matrix.data, tf_matrix.indptr\n",
    "\n",
    "    # document frequency\n",
    "    # df(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "\n",
    "    # inverse document frequency\n",
    "    for k, v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "\n",
    "\n",
    "def l2_normalize(mat):\n",
    "    \"\"\"\n",
    "    Normalize the rows of a CSR matrix by their L-2 norm.\n",
    "    :param mat:\n",
    "    \"\"\"\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0\n",
    "        for j in range(ptr[i], ptr[i + 1]):\n",
    "            rsum += val[j] ** 2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0 / np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i + 1]):\n",
    "            val[j] *= rsum\n",
    "\n",
    "\n",
    "def filter_stop_words(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short.\n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "\n",
    "\n",
    "def print_csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy,\n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "            name, mat.shape[0],\n",
    "            sum(1 if mat.indptr[i + 1] > mat.indptr[i] else 0\n",
    "                for i in range(mat.shape[0])),\n",
    "            mat.shape[1], len(np.unique(mat.indices)),\n",
    "            len(mat.data)))\n",
    "    else:\n",
    "        print(\"%s: [nrows %d, ncols %d, nnz %d]\" % (name, mat.shape[0], mat.shape[1], len(mat.data)))\n",
    "    pass\n",
    "\n",
    "\n",
    "def save_tf_idf_matrix_to_disk(tf_idf_matrix):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Main ###\n",
    "\n",
    "# file_name = 'train.dat'\n",
    "\n",
    "# docs, class_labels = get_docs_and_labels(file_name)\n",
    "# print len(docs)\n",
    "\n",
    "# tf_idf_matrix = get_tf_idf_matrix(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(training_tf_idf_vector, training_tf_idf_matrix, class_labels, k=1):\n",
    "    dot_products = training_tf_idf_vector.dot(training_tf_idf_matrix.T)\n",
    "    sims = list(zip(dot_products.indices, dot_products.data))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    print sims[:k]\n",
    "    return [class_labels[s[0]][0] for s in sims[:k] if s[1] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": [nrows 14438, ncols 62958, nnz 1563144]\n",
      "None\n",
      ": [nrows 1, ncols 62958, nnz 94]\n",
      "None\n",
      "[(3, 1.0000000000000004), (13309, 0.36089958838257113), (2254, 0.36089958838257113)]\n",
      "['5', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "testing_idx = 3\n",
    "testing_tf_idf_vector = tf_idf_matrix[testing_idx,:]\n",
    "print print_csr_info(tf_idf_matrix)\n",
    "print print_csr_info(testing_tf_idf_vector)\n",
    "# print(\"mat2:\", tf_idf_matrix[testing_idx, :20].todense(), \"\\n\")\n",
    "# print(\"mat2 copy:\", testing_tf_idf_vector[:20].todense(), \"\\n\")\n",
    "print find_nearest_neighbors(testing_tf_idf_vector, tf_idf_matrix, class_labels, k=3)\n",
    "# TODO: Remove label from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main ###\n",
    "docs, class_labels = get_docs_and_labels(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = docs[0:200]\n",
    "class_labels = class_labels[0:200]\n",
    "tf_idf_matrix = get_tf_idf_matrix(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(mat, cls, fold=1, d=10):\n",
    "    r\"\"\" Split the matrix and class info into train and test data using d-fold hold-out\n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    r = int(np.ceil(n*1.0/d))\n",
    "    mattr = []\n",
    "    clstr = []\n",
    "    # split mat and cls into d folds\n",
    "    for f in range(d):\n",
    "        if f+1 != fold:\n",
    "            mattr.append( mat[f*r: min((f+1)*r, n)] )\n",
    "            clstr.extend( cls[f*r: min((f+1)*r, n)] )\n",
    "    # join all fold matrices that are not the test matrix\n",
    "    train = sp.vstack(mattr, format='csr')\n",
    "    # extract the test matrix and class values associated with the test rows\n",
    "    test = mat[(fold-1)*r: min(fold*r, n), :]\n",
    "    clste = cls[(fold-1)*r: min(fold*r, n)]\n",
    "\n",
    "    return train, clstr, test, clste\n",
    "\n",
    "\n",
    "def classifyNames(tf_idf_matrix, class_labels, k=3, d=10):\n",
    "    r\"\"\" Classify names using c-mer frequency vector representations of the names and kNN classification with \n",
    "    cosine similarity and 10-fold cross validation\n",
    "    \"\"\"\n",
    "#     docs = [cmer(n, c) for n in names]\n",
    "#     mat = build_matrix(docs)\n",
    "#     # since we're using cosine similarity, normalize the vectors\n",
    "#     csr_l2normalize(mat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def classify(x, train, clstr, k):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "        dots = x.dot(train.T)\n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "#         print 'k===', k\n",
    "#         return [clstr[s[0]] for s in sims[:k] if s[1] > 0]\n",
    "        tc = Counter(clstr[s[0]][0] for s in sims[:k]).most_common(2)\n",
    "        if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        \n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]][0]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    \n",
    "    macc = 0.0\n",
    "    for f in range(d):\n",
    "        print '===', f\n",
    "        # split data into training and testing\n",
    "        train, clstr, test, clste = splitData(tf_idf_matrix, class_labels, f+1, d)\n",
    "#         print 'train', clstr[0]\n",
    "        # predict the class of each test sample\n",
    "        clspr = [ classify(test[i,:], train, clstr, k) for i in range(test.shape[0]) ]  \n",
    "#         print 'test', clspr\n",
    "        # compute the accuracy of the prediction\n",
    "        acc = 0.0\n",
    "        for i in range(len(clste)):\n",
    "            if clste[i][0] == clspr[i]:\n",
    "                acc += 1\n",
    "        acc /= len(clste)\n",
    "        macc += acc\n",
    "    \n",
    "#     macc = 0.0\n",
    "#     for f in range(d):\n",
    "#         print '===', f\n",
    "#         # split data into training and testing\n",
    "#         train, clstr, test, clste = splitData(tf_idf_matrix, class_labels, f+1, d)\n",
    "# #         print 'train', clstr[0]\n",
    "#         # predict the class of each test sample\n",
    "#         clspr = [ classify(test[i,:], train, clstr, k) for i in range(test.shape[0]) ]  \n",
    "# #         print 'test', clspr\n",
    "#         # compute the accuracy of the prediction\n",
    "#         acc = 0.0\n",
    "#         print f1_score(clste, clspr, average=None)\n",
    "# #         for i in range(len(clste)):\n",
    "# #             if clste[i][0] == clspr[i]:\n",
    "# #                 acc += 1\n",
    "# #         acc /= len(clste)\n",
    "# #         macc += acc\n",
    "        \n",
    "    return macc/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 0\n",
      "=== 1\n",
      "=== 2\n",
      "=== 3\n",
      "=== 4\n",
      "=== 5\n",
      "=== 6\n",
      "=== 7\n",
      "=== 8\n",
      "=== 9\n",
      "success rate==== 0.495\n"
     ]
    }
   ],
   "source": [
    "rate = classifyNames(tf_idf_matrix, class_labels, k=5, d=10)\n",
    "print 'success rate====', rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_rate = 0\n",
    "best_c = 0\n",
    "best_k = 0\n",
    "\n",
    "# print 'rate. =====', rate\n",
    "# for c in range(1, 5):\n",
    "#     print('c=%d' % c)\n",
    "#     for k in range(1, 7):\n",
    "#         rate = classifyNames(tf_idf_matrix, class_labels, c, k)\n",
    "#         if rate > highest_rate:\n",
    "#             best_c = c\n",
    "#             best_k = k\n",
    "#             highest_rate = rate\n",
    "#         print('\\tk=%d, rate=%f' % (k, rate))\n",
    "# print('best parameters: c=%d, k=%d' % (best_c, best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
